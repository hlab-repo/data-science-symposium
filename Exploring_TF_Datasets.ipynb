{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploring TF Datasets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVvt73ckG60P",
        "colab_type": "text"
      },
      "source": [
        "# Exploring TensorFlow Datasets We Will Use with Torch\n",
        "This notebook illustrates how to leverage the datasets built by TensorFlow using the code we have in the GitHub repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsBVPSMfHB-H",
        "colab_type": "text"
      },
      "source": [
        "## Set-up\n",
        "The utility and helper code we have created lives here:\n",
        "\n",
        "https://github.com/hlab-repo/data_science_symposium.git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rMrlwSxFVs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hlab-repo/data_science_symposium.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZD5ZnKUFffb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from data_science_symposium.datasets import BaseTextIterDataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irgmtzizHOBX",
        "colab_type": "text"
      },
      "source": [
        "## Exploring the base dataset\n",
        "We can load any text dataset from TensorFlow's catalog with the base `BaseTextIterDataset`. For this example, let's download the SNLI dataset and see what it looks like. You can find the relevant information required in the API call below from the datasets url (peruse the other datasets available in the left hand tab as well!):\n",
        "\n",
        "https://www.tensorflow.org/datasets/catalog/snli\n",
        "\n",
        "We need to indicate a few items in order for our `BaseTextIterDataset` instance to know which dataset to download and use. This includes the `tf_dataset` name, the epoch size (given that this is a large dataset), and which `split_` to use (test or train)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX0JbgPIFreC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = BaseTextIterDataset(\n",
        "    tf_dataset='snli', # 570k Eng. sent. pairs with labels: entailment, contradiction, and neutral\n",
        "    epoch_size=16,     # using a very small epoch size for illustration; 10k+ more typical\n",
        "    split_='test',     # load the smaller dataset for illustration\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPCY94sfg8Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = DataLoader(dataset, batch_size=2) # each pass through dataloader should have 2 batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUacwEDzIcfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for batch in dataloader:\n",
        "    pprint(batch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}